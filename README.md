# DeepLearning

General take on deep learning using python language with Tensorflow, and Keras.

***
The guide is super easy to follow, but if you have any questions, please email them to cserveairf@gmail.com or create pull requests regarding your questions.

Enjoy!!!

***

Requirements :  
<br>
<img align="left" alt="TensorFlow" width="40px" src="https://img.icons8.com/color/2x/tensorflow.png">
<img align="left" alt="Python" width="40px" src="https://img.icons8.com/color/72/python.png">
<img align="left" alt="numpy" width="90px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/NumPy_logo_2020.svg/640px-NumPy_logo_2020.svg.png">
<img align="left" alt="keras" width="40px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/512px-Keras_logo.svg.png">
<img align="left" alt="openCV" width="40px" src="https://pics.freeicons.io/uploads/icons/png/2084117441551941714-512.png">
<img align="left" alt="matplotlib" width="120px" src="https://matplotlib.org/3.1.1/_static/logo2_compressed.svg"><br>
<br>
***
## 🉑 prog1
The prog1.py file guides on how to make a deep learning model using Tensorflow, and Keras. The dataset used is a collection of images of handwritten digits 0-9, of the same dimension 28 x 28.

The number_reader.model is generated when the python code is run. So, in order to use this program, after cloning the repo, delete the model directory. 
<br>
<br>
## 🉑 prog2
   ⚠️ The datasets used in this program are more than 100MB in size, hence they are not uploaded in the directory. Please download the dataset from the given link and mention its correct path in the program to use it.<br>
<br>
This program demonstrates how we can create and load our own data. It covers taking a dataset, in this case it is a collection of images of cats and dogs  
   available here => https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765  

In this program we learn to classify the data, reshape necessary objects, extract the required features, and create the raw training dataset.  
   After that we shuffle the data, and finally save it with its features and labels separated for future use.
***

